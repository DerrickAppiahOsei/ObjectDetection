{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.ndimage import center_of_mass, maximum_position\n",
    "from scipy.ndimage import label, find_objects\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "### Counting methods ###\n",
    "# Below list six different counting methods, primaryly use connected component labeling(CCL) to find the clusters, \n",
    "# and assign the entry position to max intensity pixel, or center of mass, or center of mass after binarization, or random.\n",
    "# the last one, fastrcnn_predict, is using the ML model, instead of CCL.\n",
    "# all methods return the 256x256 counted image and coords array of shape(num, 2)\n",
    "\n",
    "\n",
    "def cca(img):\n",
    "  '''\n",
    "  only returns the stats from cca\n",
    "  '''\n",
    "  thresh = np.array(img > 20).astype('int8')\n",
    "  output = cv2.connectedComponentsWithStatsWithAlgorithm(thresh, 8, cv2.CV_32S, 0) \n",
    "  (_, _, stats, centroids) = output\n",
    "  return stats\n",
    "\n",
    "\n",
    "def counting_filter_binary_com(image, threshold=20, structure = np.ones((3,3))):\n",
    "    image_binary = image > threshold  # more readable\n",
    "    all_labels, num = label(image_binary, structure = np.ones((3,3)))  # get blobs\n",
    "    m=np.ones(shape=all_labels.shape)\n",
    "    obj = center_of_mass(m, all_labels, range(1,num))\n",
    "    obj = np.rint(obj).astype(int)\n",
    "    x = np.zeros(shape=np.shape(image))\n",
    "    x[obj[:,0],obj[:,1]]=1\n",
    "    return x, obj\n",
    "\n",
    "\n",
    "def counting_filter_com(image, threshold=20, structure = np.ones((3,3))):\n",
    "    image_binary = image > threshold  # more readable\n",
    "    all_labels, num = label(image_binary, structure = np.ones((3,3)))  # get blobs\n",
    "    # m=np.ones(shape=all_labels.shape)\n",
    "    obj = center_of_mass(image, all_labels, range(1,num))\n",
    "    obj = np.rint(obj).astype(int)\n",
    "    x = np.zeros(shape=np.shape(image))\n",
    "    x[obj[:,0],obj[:,1]]=1\n",
    "    return x, obj\n",
    "\n",
    "\n",
    "def counting_filter_max(image, threshold=20, structure = np.ones((3,3))):\n",
    "    eventsize = []\n",
    "    image_binary = image > threshold  # more readable\n",
    "    all_labels, num = label(image_binary, structure = np.ones((3,3)))  # get blobs\n",
    "    m=np.ones(shape=all_labels.shape)\n",
    "    obj = maximum_position(image, all_labels, range(1,num))\n",
    "    obj = np.rint(obj).astype(int)\n",
    "    x = np.zeros(shape=np.shape(image))\n",
    "    x[obj[:,0],obj[:,1]]=1\n",
    "    for i in np.arange(num)[1:]:\n",
    "      eventsize.append(np.where(all_labels==i)[0].shape[0])\n",
    "    return x, obj, np.array(eventsize).astype('int')\n",
    "\n",
    "\n",
    "def counting_filter_random(image, threshold=20, structure = np.ones((3,3))):\n",
    "    image_binary = image > threshold  # more readable\n",
    "    all_labels, num = label(image_binary, structure = np.ones((3,3)))  # get blobs\n",
    "    obj = find_objects(all_labels)\n",
    "    coords = []\n",
    "    for i in range(len(obj)):\n",
    "      coords.append((np.random.randint(obj[i][0].start,obj[i][0].stop),\n",
    "                    np.random.randint(obj[i][1].start,obj[i][1].stop)))\n",
    "    coords = np.array(coords)\n",
    "    x = np.zeros(shape=np.shape(image))\n",
    "    x[coords[:,0], coords[:,1]] = 1\n",
    "    return x, coords\n",
    "\n",
    "\n",
    "def fastrcnn_predict(model, arr, process_stride, mode, **kwargs):\n",
    "  from CountingNN.locator import Locator\n",
    "  x = arr[None, ...]\n",
    "  device =  torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "  counting = Locator(model, device, process_stride, 'max', 30, None, mode, meanADU = kwargs.get('meanADU'), \n",
    "                     p_list=kwargs.get('p_list'), dynamic_thres = kwargs.get('dynamic_thres'), pretune_thresholding = kwargs.get('pretune_thresholding'))\n",
    "  filtered, event_sizes =  counting.predict_sequence(x)\n",
    "  filtered = filtered[0]\n",
    "  all_coords = []\n",
    "  for value in range(1, 1 + filtered.max()):\n",
    "      coords = np.array(np.where(filtered==value))        \n",
    "      all_coords.append([coords]*value)\n",
    "  all_coords = np.hstack(np.array(all_coords)[0]).T\n",
    "\n",
    "  return filtered, all_coords, event_sizes\n",
    "\n",
    "##########################################################################################################################\n",
    "### Evaluation metrics calculation ###\n",
    "\n",
    "def pos_deviation(coords, truth, threshold):\n",
    "    \"\"\"\n",
    "    Cal the root mean square error between detected electron incident positions and the ground truth positions in units of pixels.\n",
    "    \"\"\"\n",
    "    # elements in pair 1 need to be no less than pair 2 \n",
    "    distances = []\n",
    "    if len(coords):\n",
    "      assigment,distances = pairwise_distances_argmin_min(coords, truth)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def general_evaluation(file, algorithm, repeat, savepath, **kwargs):\n",
    "  '''\n",
    "  This function is for calculating the scores for each data file, Stack***.npz together. \n",
    "  Arguments\n",
    "  -----------\n",
    "  file: the validation data file. e.g., my Stack000.npz contains images with sparsity 0~0.002, array X is the input images, \n",
    "  it has the shape of [N, M, 256, 256], N different sparsity ranging from sparsitymin(0) to sparsitymax(0.002), M copies of each same sparsity.\n",
    "  Then Stack001.npz contains images with sparsity 0.002~0.004, and so on. \n",
    "\n",
    "  algorithm: run evaluation of one counting algorithm, string of the function name defined above.\n",
    "\n",
    "  repeat: set the number of images with identical sparsity in each data file to be used.  \n",
    "  '''\n",
    "  data = np.load(file)\n",
    "\n",
    "  X = data['X'][:,:repeat]\n",
    "  y = data['y'][:,:repeat]\n",
    "  print('Max pixel value in ground truth:', y.max())\n",
    "  # creat blank arrays to store the score values, which has the same first two dimension as X. \n",
    "  dce = np.zeros(X.shape[:2]) # dce: simple detector conversion efficiency,  the ratio of input and detected electron counts\n",
    "  mae = np.zeros(X.shape[:2]) # mae: mean absolute error, the absolute error of electron counts averaged over all pixels in a single image\n",
    "  nume = np.zeros(X.shape[:2]) # number of actrual electrons in the image\n",
    "  recall = np.zeros(X.shape[:2]) # recall, true positive / (true positives + false negtives)\n",
    "  precision = np.zeros(X.shape[:2]) # precision, true positive / (true positives + false positives)\n",
    "  filtered =  np.zeros(X.shape) # i.e. the counted image\n",
    "\n",
    "  # saving the coordinate, position deviation and event size for each detected electron event in the image, \n",
    "  # so need to create an array of objects, and the object is a list\n",
    "  coords = [ [0] * X.shape[1] ] * X.shape[0]\n",
    "  deviations = [ [0] * X.shape[1] ] * X.shape[0]\n",
    "  eventsizes = [ [0] * X.shape[1] ] * X.shape[0]\n",
    "  coords = np.array(coords, dtype=object)\n",
    "  deviations = np.array(deviations, dtype=object)\n",
    "  eventsizes = np.array(eventsizes, dtype=object)\n",
    "  save_e_size = True\n",
    "\n",
    "\n",
    "  # Now go through the NxM images to get the scores\n",
    "  for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "          \n",
    "      if algorithm =='fastrcnn_predict':\n",
    "        model = kwargs.get('model')\n",
    "        method = kwargs.get('method')\n",
    "        stride = kwargs.get('stride')\n",
    "        mode = kwargs.get('mode')\n",
    "        meanADU = kwargs.get('meanADU')\n",
    "        p_list = kwargs.get('p_list')\n",
    "        dynamic_thres = kwargs.get('dynamic_thres')\n",
    "        pretune_thresholding = kwargs.get('pretune_thresholding')\n",
    "        # by using the \"eval\", the long string has been read as a line of code, and it runs the algorithm function\n",
    "        res = eval(algorithm +\"(model, X[i][j], stride, mode, meanADU=meanADU, p_list=p_list, dynamic_thres = dynamic_thres,pretune_thresholding = pretune_thresholding )\")\n",
    "        filtered[i,j] = res[0]\n",
    "        coords[i][j] = res[1]\n",
    "        # if the algorithm returns eventsize, set to save it.\n",
    "        try:\n",
    "          eventsizes[i][j] = res[2]\n",
    "        except:\n",
    "          save_e_size = False\n",
    "\n",
    "      else: \n",
    "        res = eval(algorithm + \"(X[i][j])\")\n",
    "        filtered[i,j] = res[0]\n",
    "        coords[i][j] = res[1]\n",
    "        try:\n",
    "          eventsizes[i][j] = res[2]\n",
    "        except:\n",
    "          save_e_size = False\n",
    "      \n",
    "      # Get all the ground truth coordinates of electron events\n",
    "      # For a pixel value 2 for example, indicating 2 electrons here, so we need to add its coordinate twice. \n",
    "      truth = []\n",
    "      for value in range(1, 1+int(y[i,j].max())):\n",
    "        truth_ = np.array(np.where(y[i,j]==value))        \n",
    "        truth.append([truth_]*value)\n",
    "      truth = np.hstack(np.array(truth)[0]).T\n",
    "\n",
    "      total_pixel = filtered[i,j].shape[0] * filtered[i,j].shape[1]\n",
    "      mae[i,j] = np.sum(np.abs(filtered[i,j]-y[i,j]))/total_pixel\n",
    "      dce[i,j] = np.sum(filtered[i,j])/np.sum(y[i,j])\n",
    "      nume[i,j] = np.sum(y[i,j])\n",
    "\n",
    "      tp = 0 \n",
    "      # count how many electron events are well identified, i.e., count the true positives\n",
    "      for n, value in enumerate(filtered[i,j].ravel()):\n",
    "\n",
    "        if (value != 0) & (y[i,j].ravel()[n] != 0):\n",
    "          tp = tp + np.min((value, y[i,j].ravel()[n])) # multi-class considered\n",
    "\n",
    "      recall[i,j] = tp/nume[i,j]\n",
    "      precision[i,j] = tp/np.sum(filtered[i,j])\n",
    "\n",
    "      deviations[i][j] = pos_deviation(coords[i][j], truth, 6)\n",
    "      dce[i,j] = len(deviations[i][j])/np.sum(y[i,j])\n",
    "\n",
    "  path = savepath + file[-12:-4]\n",
    "  if save_e_size:\n",
    "    np.savez(path+'_result.npz', coordinates = coords, result = filtered, mae = mae, dce = dce, nume = nume, \n",
    "    recall = recall, precision = precision, deviations = deviations, eventsizes = eventsizes)\n",
    "  else:\n",
    "    np.savez(path+'_result.npz', coordinates = coords, result = filtered, mae = mae, dce = dce, nume = nume, \n",
    "    recall = recall, precision = precision, deviations = deviations)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
